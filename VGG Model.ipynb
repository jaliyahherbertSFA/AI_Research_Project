{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "70940fd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import cv2\n",
    "import tensorflow as tf\n",
    "import pathlib\n",
    "from PIL import Image\n",
    "from numpy import asarray\n",
    "#import tensorflow_datasets as tfds\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import datasets, layers, models, metrics\n",
    "from tensorflow.keras.layers import Input, Conv2D, Dense, Flatten, Activation, MaxPool2D, BatchNormalization, Dropout\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime\n",
    "from sklearn.model_selection import KFold, StratifiedKFold\n",
    "import sklearn.metrics as metrics\n",
    "from IPython.display import Image, display\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c1a2e820",
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path = '/home/jaliyah/data'\n",
    "biofilm_full_path = os.path.join(folder_path, 'biofilm')\n",
    "nonbio_full_path = os.path.join(folder_path, 'single_cell')\n",
    "my_bio_files_list = []\n",
    "my_non_files_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "042ec479",
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in os.listdir(biofilm_full_path):\n",
    "    my_bio_files_list.append(os.path.join(biofilm_full_path,file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a76091b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_width = 300\n",
    "img_height = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5a1e9514",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(276, 300, 300, 3)\n"
     ]
    }
   ],
   "source": [
    "total_img_bio = np.zeros((276,img_width,img_height,3), dtype=np.int16)\n",
    "biofilm_label= np.ones(276)\n",
    "print(total_img_bio.shape)\n",
    "for (i,file) in enumerate(my_bio_files_list):\n",
    "    img = cv2.imread(file)\n",
    "   # print(file)\n",
    "   # plt.imshow(img)\n",
    "   # plt.imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))\n",
    "    img_1 = img.resize(img_width,img_height,3)\n",
    "    total_img_bio[i] = img\n",
    "   # print(biofilm_label[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5027b843",
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in os.listdir(nonbio_full_path):\n",
    "    my_non_files_list.append(os.path.join(nonbio_full_path,file))\n",
    "#print((my_non_files_list[63]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0cc7c6d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(190, 300, 300, 3)\n"
     ]
    }
   ],
   "source": [
    "total_img_non = np.zeros((190,img_width,img_height,3), dtype=np.int16)\n",
    "print(total_img_non.shape)\n",
    "non_biofilm_label= np.zeros(190)\n",
    "\n",
    "for (i,file) in enumerate(my_non_files_list):\n",
    "    img = cv2.imread(file)\n",
    "   # print(os.path.exists(file))\n",
    "    #print(file)\n",
    "    img_1 = img.resize(img_width,img_height,3)\n",
    "    #print(file)\n",
    "    #print (img.shape)\n",
    "    #print(img)\n",
    "    total_img_non[i] = img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "be3205da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(55,)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "total = total_img_bio.shape[0]\n",
    "size = int(total * .20)\n",
    "test_bio = random.sample(range(0, total), size)\n",
    "np.unique(test_bio).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8e7c23fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(300, 300, 3)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_bio_img = total_img_bio[test_bio]\n",
    "test_bio_lbl = biofilm_label[test_bio]\n",
    "test_bio_img[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bfdbc937",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(38,)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total = total_img_non.shape[0]\n",
    "size = int(total * .20)\n",
    "test_non = random.sample(range(0, total), size)\n",
    "np.unique(test_non).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "555cfe43",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_non_img = total_img_non[test_non]\n",
    "test_non_lbl = non_biofilm_label[test_non]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "95961e0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_test_img = np.concatenate([test_bio_img,test_non_img])\n",
    "total_test_lbl = np.concatenate([test_bio_lbl,test_non_lbl])\n",
    "\n",
    "#np.save('Dataset/test_img.npy', total_test_img) \n",
    "#np.save('Dataset/test_lbl.npy', total_test_lbl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "24d57a58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "221\n"
     ]
    }
   ],
   "source": [
    "a = list(range(total_img_bio.shape[0]))\n",
    "#gives us total 2225 to use for train/vali\n",
    "train_vali_total = (set(a) -(set(test_bio)))\n",
    "len_train_vali_total = (len(train_vali_total))\n",
    "print(len_train_vali_total)\n",
    "size = int(len_train_vali_total * .10)\n",
    "#size\n",
    "vail_bio = random.sample(range(0, len_train_vali_total), size)\n",
    "np.unique(vail_bio).shape\n",
    "vail_bio_img = total_img_bio[vail_bio]\n",
    "vail_bio_lbl = biofilm_label[vail_bio]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "48abcefe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(199,)\n"
     ]
    }
   ],
   "source": [
    "a = list(range(len_train_vali_total))\n",
    "train_total = (set(a) -(set(vail_bio)))\n",
    "train_bio = list(train_total)\n",
    "train_bio_img = total_img_bio[train_bio]\n",
    "train_bio_lbl = biofilm_label[train_bio]\n",
    "train_bio_img.shape\n",
    "x = np.array(train_bio)\n",
    "print(np.unique(x).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "872c36d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "152\n",
      "15\n"
     ]
    }
   ],
   "source": [
    "a = list(range(total_img_non.shape[0]))\n",
    "#gives us total 4897 to use for train/vali\n",
    "train_vali_non_total = (set(a) -(set(test_non)))\n",
    "len_train_vali_non_total = (len(train_vali_non_total))\n",
    "print(len_train_vali_non_total)\n",
    "size = int(len_train_vali_non_total * .10)\n",
    "print(size)\n",
    "vail_non = random.sample(range(0, len_train_vali_non_total), size)\n",
    "vail_non_img = total_img_non[vail_non]\n",
    "vail_non_lbl = non_biofilm_label[vail_non]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "53b46b6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = list(range(len_train_vali_non_total))\n",
    "train_non_total = (set(a) -(set(vail_non)))\n",
    "train_non = list(train_non_total)\n",
    "train_non_img = total_img_non[train_non]\n",
    "train_non_lbl = non_biofilm_label[train_non]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "094c12e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_vali_img = np.concatenate([vail_bio_img,vail_non_img])\n",
    "total_vali_lbl = np.concatenate([vail_bio_lbl,vail_non_lbl])\n",
    "#np.save('Dataset/vali_img.npy', total_vali_img)\n",
    "#np.save('Dataset/vali_lbl.npy', total_vali_lbl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fc2ab21b",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_train_img = np.concatenate([train_bio_img,train_non_img])\n",
    "total_train_lbl = np.concatenate([train_bio_lbl,train_non_lbl])\n",
    "#np.save('Dataset/train_img.npy', total_train_img)\n",
    "#np.save('Dataset/train_lbl.npy', total_train_lbl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "73e7470e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_datagen = keras.preprocessing.image.ImageDataGenerator(\n",
    "        rotation_range=20,\n",
    "        zoom_range=0.15,\n",
    "        width_shift_range=0.2,\n",
    "        height_shift_range=0.2,\n",
    "        shear_range=0.15,\n",
    "        horizontal_flip=True,\n",
    "        fill_mode=\"nearest\")\n",
    "\n",
    "test_datagen = keras.preprocessing.image.ImageDataGenerator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d33ebeef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(input_shape):\n",
    "    inputs = Input(shape = input_shape)\n",
    "    #1st block\n",
    "    x = Conv2D (filters =64, kernel_size =3, padding ='same', activation='relu')(inputs)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Conv2D (filters =64, kernel_size =3, padding ='same', activation='relu')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = MaxPool2D(pool_size =2, strides =2, padding ='same')(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "    #2nd block\n",
    "    x = Conv2D (filters =128, kernel_size =3, padding ='same', activation='relu')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Conv2D (filters =128, kernel_size =3, padding ='same', activation='relu')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = MaxPool2D(pool_size =2, strides =2, padding ='same')(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "    #3rd block\n",
    "    x = Conv2D (filters =256, kernel_size =3, padding ='same', activation='relu')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Conv2D (filters =256, kernel_size =3, padding ='same', activation='relu')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Conv2D (filters =256, kernel_size =3, padding ='same', activation='relu')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = MaxPool2D(pool_size =2, strides =2, padding ='same')(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "    #4th block\n",
    "    \"\"\"\n",
    "    x = Conv2D (filters =512, kernel_size =3, padding ='same', activation='relu')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Conv2D (filters =512, kernel_size =3, padding ='same', activation='relu')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Conv2D (filters =512, kernel_size =3, padding ='same', activation='relu')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = MaxPool2D(pool_size =2, strides =2, padding ='same')(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "    \"\"\"\n",
    "    # 5th Conv block\n",
    "\n",
    "    x = Conv2D (filters =512, kernel_size =3, padding ='same', activation='relu')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Conv2D (filters =512, kernel_size =3, padding ='same', activation='relu')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Conv2D (filters =512, kernel_size =3, padding ='same', activation='relu')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = MaxPool2D(pool_size =2, strides =2, padding ='same')(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "    # Fully connected layers  \n",
    "    x = Flatten()(x) \n",
    "    x = Dense(units = 1024, activation ='relu', kernel_regularizer = 'l2')(x) \n",
    "    x = Dropout(0.3)(x)\n",
    "    x = Dense(units = 1024, activation ='relu')(x) \n",
    "    x = Dropout(0.3)(x)\n",
    "    outputs = Dense(units = 1, activation ='sigmoid')(x)\n",
    "    return keras.Model(inputs,outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a2e6f100",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_name(k):\n",
    "    return 'model_'+str(k)+'.h5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5c452ab4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-08-01 17:28:18.860228: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-08-01 17:28:18.895283: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-08-01 17:28:18.895522: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-08-01 17:28:18.896702: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-08-01 17:28:18.900591: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-08-01 17:28:18.900874: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-08-01 17:28:18.901116: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-08-01 17:28:19.283325: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-08-01 17:28:19.283527: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-08-01 17:28:19.283682: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-08-01 17:28:19.283843: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 45777 MB memory:  -> device: 0, name: NVIDIA RTX A6000, pci bus id: 0000:21:00.0, compute capability: 8.6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-08-01 17:28:22.545139: I tensorflow/stream_executor/cuda/cuda_dnn.cc:366] Loaded cuDNN version 8300\n",
      "2022-08-01 17:28:23.561152: W tensorflow/stream_executor/gpu/asm_compiler.cc:111] *** WARNING *** You are using ptxas 10.1.243, which is older than 11.1. ptxas before 11.1 is known to miscompile XLA code, leading to incorrect results or invalid-address errors.\n",
      "\n",
      "You may not need to update to CUDA 11.1; cherry-picking the ptxas binary is often sufficient.\n",
      "2022-08-01 17:28:23.563186: W tensorflow/stream_executor/gpu/asm_compiler.cc:230] Falling back to the CUDA driver for PTX compilation; ptxas does not support CC 8.6\n",
      "2022-08-01 17:28:23.563220: W tensorflow/stream_executor/gpu/asm_compiler.cc:233] Used ptxas at ptxas\n",
      "2022-08-01 17:28:23.563344: W tensorflow/stream_executor/gpu/redzone_allocator.cc:314] UNIMPLEMENTED: ptxas ptxas too old. Falling back to the driver to compile.\n",
      "Relying on driver to perform ptx compilation. \n",
      "Modify $PATH to customize ptxas location.\n",
      "This message will be only logged once.\n",
      "2022-08-01 17:28:24.484727: I tensorflow/stream_executor/cuda/cuda_blas.cc:1774] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/10 [==============================] - ETA: 0s - loss: 20.9957 - accuracy: 0.7919 - f1_m: 0.8350 - precision_m: 0.8430 - recall_m: 0.8626"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-08-01 17:28:32.714044: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 45424312320 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00001: val_accuracy improved from -inf to 0.54667, saving model to saved_models/model_1.h5\n",
      "10/10 [==============================] - 28s 3s/step - loss: 20.9957 - accuracy: 0.7919 - f1_m: 0.8350 - precision_m: 0.8430 - recall_m: 0.8626 - val_loss: 21.6981 - val_accuracy: 0.5467 - val_f1_m: 0.6880 - val_precision_m: 0.5265 - val_recall_m: 1.0000\n",
      "Epoch 2/50\n",
      "10/10 [==============================] - ETA: 0s - loss: 20.6439 - accuracy: 0.8993 - f1_m: 0.9124 - precision_m: 0.9462 - recall_m: 0.8956"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-08-01 17:28:54.602885: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 45424312320 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00002: val_accuracy did not improve from 0.54667\n",
      "10/10 [==============================] - 13s 1s/step - loss: 20.6439 - accuracy: 0.8993 - f1_m: 0.9124 - precision_m: 0.9462 - recall_m: 0.8956 - val_loss: 20.9759 - val_accuracy: 0.5467 - val_f1_m: 0.6367 - val_precision_m: 0.4867 - val_recall_m: 1.0000\n",
      "Epoch 3/50\n",
      "10/10 [==============================] - ETA: 0s - loss: 20.6215 - accuracy: 0.8960 - f1_m: 0.9158 - precision_m: 0.8977 - recall_m: 0.9470"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-08-01 17:29:07.627799: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 45424312320 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00003: val_accuracy improved from 0.54667 to 0.82667, saving model to saved_models/model_1.h5\n",
      "10/10 [==============================] - 23s 2s/step - loss: 20.6215 - accuracy: 0.8960 - f1_m: 0.9158 - precision_m: 0.8977 - recall_m: 0.9470 - val_loss: 20.7600 - val_accuracy: 0.8267 - val_f1_m: 0.7962 - val_precision_m: 1.0000 - val_recall_m: 0.6630\n",
      "Epoch 4/50\n",
      "10/10 [==============================] - ETA: 0s - loss: 20.4080 - accuracy: 0.9430 - f1_m: 0.9525 - precision_m: 0.9637 - recall_m: 0.9444"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-08-01 17:29:30.335345: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 45424312320 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00004: val_accuracy did not improve from 0.82667\n",
      "10/10 [==============================] - 13s 1s/step - loss: 20.4080 - accuracy: 0.9430 - f1_m: 0.9525 - precision_m: 0.9637 - recall_m: 0.9444 - val_loss: 21.1693 - val_accuracy: 0.4533 - val_f1_m: 0.0000e+00 - val_precision_m: 0.0000e+00 - val_recall_m: 0.0000e+00\n",
      "Epoch 5/50\n",
      "10/10 [==============================] - ETA: 0s - loss: 20.3148 - accuracy: 0.9362 - f1_m: 0.9443 - precision_m: 0.9345 - recall_m: 0.9578"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-08-01 17:29:43.515256: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 45424312320 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00005: val_accuracy did not improve from 0.82667\n",
      "10/10 [==============================] - 13s 1s/step - loss: 20.3148 - accuracy: 0.9362 - f1_m: 0.9443 - precision_m: 0.9345 - recall_m: 0.9578 - val_loss: 21.6645 - val_accuracy: 0.4533 - val_f1_m: 0.0000e+00 - val_precision_m: 0.0000e+00 - val_recall_m: 0.0000e+00\n",
      "Epoch 6/50\n",
      "10/10 [==============================] - ETA: 0s - loss: 20.1877 - accuracy: 0.9497 - f1_m: 0.9522 - precision_m: 0.9590 - recall_m: 0.9512\n",
      "Epoch 00006: val_accuracy did not improve from 0.82667\n",
      "10/10 [==============================] - 13s 1s/step - loss: 20.1877 - accuracy: 0.9497 - f1_m: 0.9522 - precision_m: 0.9590 - recall_m: 0.9512 - val_loss: 22.1484 - val_accuracy: 0.4533 - val_f1_m: 0.0000e+00 - val_precision_m: 0.0000e+00 - val_recall_m: 0.0000e+00\n",
      "Epoch 7/50\n",
      "10/10 [==============================] - ETA: 0s - loss: 20.0363 - accuracy: 0.9799 - f1_m: 0.9844 - precision_m: 0.9806 - recall_m: 0.9892\n",
      "Epoch 00007: val_accuracy did not improve from 0.82667\n",
      "10/10 [==============================] - 13s 1s/step - loss: 20.0363 - accuracy: 0.9799 - f1_m: 0.9844 - precision_m: 0.9806 - recall_m: 0.9892 - val_loss: 22.6252 - val_accuracy: 0.4533 - val_f1_m: 0.0000e+00 - val_precision_m: 0.0000e+00 - val_recall_m: 0.0000e+00\n",
      "Epoch 8/50\n",
      "10/10 [==============================] - ETA: 0s - loss: 20.0095 - accuracy: 0.9564 - f1_m: 0.9561 - precision_m: 0.9383 - recall_m: 0.9781\n",
      "Epoch 00008: val_accuracy did not improve from 0.82667\n",
      "10/10 [==============================] - 13s 1s/step - loss: 20.0095 - accuracy: 0.9564 - f1_m: 0.9561 - precision_m: 0.9383 - recall_m: 0.9781 - val_loss: 23.0286 - val_accuracy: 0.4533 - val_f1_m: 0.0000e+00 - val_precision_m: 0.0000e+00 - val_recall_m: 0.0000e+00\n",
      "3/3 [==============================] - 1s 211ms/step - loss: 20.7488 - accuracy: 0.8400 - f1_m: 0.8111 - precision_m: 0.9744 - recall_m: 0.7000\n",
      "Epoch 1/50\n",
      "10/10 [==============================] - ETA: 0s - loss: 21.0186 - accuracy: 0.7819 - f1_m: 0.8213 - precision_m: 0.8399 - recall_m: 0.8321\n",
      "Epoch 00001: val_accuracy improved from -inf to 0.56000, saving model to saved_models/model_2.h5\n",
      "10/10 [==============================] - 23s 2s/step - loss: 21.0186 - accuracy: 0.7819 - f1_m: 0.8213 - precision_m: 0.8399 - recall_m: 0.8321 - val_loss: 21.4508 - val_accuracy: 0.5600 - val_f1_m: 0.7122 - val_precision_m: 0.5568 - val_recall_m: 1.0000\n",
      "Epoch 2/50\n",
      "10/10 [==============================] - ETA: 0s - loss: 20.5562 - accuracy: 0.9094 - f1_m: 0.9243 - precision_m: 0.9313 - recall_m: 0.9265\n",
      "Epoch 00002: val_accuracy did not improve from 0.56000\n",
      "10/10 [==============================] - 13s 1s/step - loss: 20.5562 - accuracy: 0.9094 - f1_m: 0.9243 - precision_m: 0.9313 - recall_m: 0.9265 - val_loss: 20.9879 - val_accuracy: 0.5600 - val_f1_m: 0.7153 - val_precision_m: 0.5568 - val_recall_m: 1.0000\n",
      "Epoch 3/50\n",
      "10/10 [==============================] - ETA: 0s - loss: 20.5741 - accuracy: 0.8960 - f1_m: 0.9105 - precision_m: 0.9047 - recall_m: 0.9309\n",
      "Epoch 00003: val_accuracy did not improve from 0.56000\n",
      "10/10 [==============================] - 13s 1s/step - loss: 20.5741 - accuracy: 0.8960 - f1_m: 0.9105 - precision_m: 0.9047 - recall_m: 0.9309 - val_loss: 20.8960 - val_accuracy: 0.4800 - val_f1_m: 0.1952 - val_precision_m: 0.5167 - val_recall_m: 0.1205\n",
      "Epoch 4/50\n",
      "10/10 [==============================] - ETA: 0s - loss: 20.3679 - accuracy: 0.9295 - f1_m: 0.9328 - precision_m: 0.9322 - recall_m: 0.9424\n",
      "Epoch 00004: val_accuracy did not improve from 0.56000\n",
      "10/10 [==============================] - 13s 1s/step - loss: 20.3679 - accuracy: 0.9295 - f1_m: 0.9328 - precision_m: 0.9322 - recall_m: 0.9424 - val_loss: 21.2405 - val_accuracy: 0.4400 - val_f1_m: 0.0000e+00 - val_precision_m: 0.0000e+00 - val_recall_m: 0.0000e+00\n",
      "Epoch 5/50\n",
      "10/10 [==============================] - ETA: 0s - loss: 20.2931 - accuracy: 0.9195 - f1_m: 0.9320 - precision_m: 0.9667 - recall_m: 0.9086\n",
      "Epoch 00005: val_accuracy did not improve from 0.56000\n",
      "10/10 [==============================] - 13s 1s/step - loss: 20.2931 - accuracy: 0.9195 - f1_m: 0.9320 - precision_m: 0.9667 - recall_m: 0.9086 - val_loss: 21.8049 - val_accuracy: 0.4400 - val_f1_m: 0.0000e+00 - val_precision_m: 0.0000e+00 - val_recall_m: 0.0000e+00\n",
      "Epoch 6/50\n",
      "10/10 [==============================] - ETA: 0s - loss: 20.3159 - accuracy: 0.9161 - f1_m: 0.9396 - precision_m: 0.9187 - recall_m: 0.9701\n",
      "Epoch 00006: val_accuracy did not improve from 0.56000\n",
      "10/10 [==============================] - 13s 1s/step - loss: 20.3159 - accuracy: 0.9161 - f1_m: 0.9396 - precision_m: 0.9187 - recall_m: 0.9701 - val_loss: 22.2412 - val_accuracy: 0.4400 - val_f1_m: 0.0000e+00 - val_precision_m: 0.0000e+00 - val_recall_m: 0.0000e+00\n",
      "Epoch 7/50\n",
      "10/10 [==============================] - ETA: 0s - loss: 20.1856 - accuracy: 0.9362 - f1_m: 0.9513 - precision_m: 0.9518 - recall_m: 0.9582\n",
      "Epoch 00007: val_accuracy did not improve from 0.56000\n",
      "10/10 [==============================] - 13s 1s/step - loss: 20.1856 - accuracy: 0.9362 - f1_m: 0.9513 - precision_m: 0.9518 - recall_m: 0.9582 - val_loss: 22.6988 - val_accuracy: 0.4400 - val_f1_m: 0.0000e+00 - val_precision_m: 0.0000e+00 - val_recall_m: 0.0000e+00\n",
      "Epoch 8/50\n",
      "10/10 [==============================] - ETA: 0s - loss: 20.1938 - accuracy: 0.9195 - f1_m: 0.9169 - precision_m: 0.9547 - recall_m: 0.8992\n",
      "Epoch 00008: val_accuracy did not improve from 0.56000\n",
      "10/10 [==============================] - 13s 1s/step - loss: 20.1938 - accuracy: 0.9195 - f1_m: 0.9169 - precision_m: 0.9547 - recall_m: 0.8992 - val_loss: 23.0914 - val_accuracy: 0.4400 - val_f1_m: 0.0000e+00 - val_precision_m: 0.0000e+00 - val_recall_m: 0.0000e+00\n",
      "3/3 [==============================] - 1s 218ms/step - loss: 21.4520 - accuracy: 0.5600 - f1_m: 0.6960 - precision_m: 0.5369 - recall_m: 1.0000\n",
      "Epoch 1/50\n",
      "10/10 [==============================] - ETA: 0s - loss: 20.9277 - accuracy: 0.7886 - f1_m: 0.8022 - precision_m: 0.8178 - recall_m: 0.8155\n",
      "Epoch 00001: val_accuracy improved from -inf to 0.58667, saving model to saved_models/model_3.h5\n",
      "10/10 [==============================] - 24s 2s/step - loss: 20.9277 - accuracy: 0.7886 - f1_m: 0.8022 - precision_m: 0.8178 - recall_m: 0.8155 - val_loss: 22.1875 - val_accuracy: 0.5867 - val_f1_m: 0.7125 - val_precision_m: 0.5578 - val_recall_m: 1.0000\n",
      "Epoch 2/50\n",
      "10/10 [==============================] - ETA: 0s - loss: 20.7437 - accuracy: 0.8523 - f1_m: 0.8736 - precision_m: 0.9092 - recall_m: 0.8655\n",
      "Epoch 00002: val_accuracy did not improve from 0.58667\n",
      "10/10 [==============================] - 13s 1s/step - loss: 20.7437 - accuracy: 0.8523 - f1_m: 0.8736 - precision_m: 0.9092 - recall_m: 0.8655 - val_loss: 21.3299 - val_accuracy: 0.5867 - val_f1_m: 0.7131 - val_precision_m: 0.5578 - val_recall_m: 1.0000\n",
      "Epoch 3/50\n",
      "10/10 [==============================] - ETA: 0s - loss: 20.7054 - accuracy: 0.9128 - f1_m: 0.9255 - precision_m: 0.9010 - recall_m: 0.9624\n",
      "Epoch 00003: val_accuracy improved from 0.58667 to 0.76000, saving model to saved_models/model_3.h5\n",
      "10/10 [==============================] - 22s 2s/step - loss: 20.7054 - accuracy: 0.9128 - f1_m: 0.9255 - precision_m: 0.9010 - recall_m: 0.9624 - val_loss: 20.8080 - val_accuracy: 0.7600 - val_f1_m: 0.8202 - val_precision_m: 0.7052 - val_recall_m: 0.9815\n",
      "Epoch 4/50\n",
      "10/10 [==============================] - ETA: 0s - loss: 20.4030 - accuracy: 0.9362 - f1_m: 0.9435 - precision_m: 0.9541 - recall_m: 0.9397\n",
      "Epoch 00004: val_accuracy did not improve from 0.76000\n",
      "10/10 [==============================] - 13s 1s/step - loss: 20.4030 - accuracy: 0.9362 - f1_m: 0.9435 - precision_m: 0.9541 - recall_m: 0.9397 - val_loss: 21.1407 - val_accuracy: 0.4133 - val_f1_m: 0.0000e+00 - val_precision_m: 0.0000e+00 - val_recall_m: 0.0000e+00\n",
      "Epoch 5/50\n",
      "10/10 [==============================] - ETA: 0s - loss: 20.3803 - accuracy: 0.9161 - f1_m: 0.9058 - precision_m: 0.9184 - recall_m: 0.9151\n",
      "Epoch 00005: val_accuracy did not improve from 0.76000\n",
      "10/10 [==============================] - 13s 1s/step - loss: 20.3803 - accuracy: 0.9161 - f1_m: 0.9058 - precision_m: 0.9184 - recall_m: 0.9151 - val_loss: 22.1354 - val_accuracy: 0.4133 - val_f1_m: 0.0000e+00 - val_precision_m: 0.0000e+00 - val_recall_m: 0.0000e+00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/50\n",
      "10/10 [==============================] - ETA: 0s - loss: 20.3052 - accuracy: 0.9396 - f1_m: 0.9419 - precision_m: 0.9243 - recall_m: 0.9659\n",
      "Epoch 00006: val_accuracy did not improve from 0.76000\n",
      "10/10 [==============================] - 13s 1s/step - loss: 20.3052 - accuracy: 0.9396 - f1_m: 0.9419 - precision_m: 0.9243 - recall_m: 0.9659 - val_loss: 22.9534 - val_accuracy: 0.4133 - val_f1_m: 0.0000e+00 - val_precision_m: 0.0000e+00 - val_recall_m: 0.0000e+00\n",
      "Epoch 7/50\n",
      "10/10 [==============================] - ETA: 0s - loss: 20.2008 - accuracy: 0.9396 - f1_m: 0.9538 - precision_m: 0.9537 - recall_m: 0.9580\n",
      "Epoch 00007: val_accuracy did not improve from 0.76000\n",
      "10/10 [==============================] - 13s 1s/step - loss: 20.2008 - accuracy: 0.9396 - f1_m: 0.9538 - precision_m: 0.9537 - recall_m: 0.9580 - val_loss: 23.6092 - val_accuracy: 0.4133 - val_f1_m: 0.0000e+00 - val_precision_m: 0.0000e+00 - val_recall_m: 0.0000e+00\n",
      "Epoch 8/50\n",
      "10/10 [==============================] - ETA: 0s - loss: 20.1423 - accuracy: 0.9396 - f1_m: 0.9441 - precision_m: 0.9550 - recall_m: 0.9396\n",
      "Epoch 00008: val_accuracy did not improve from 0.76000\n",
      "10/10 [==============================] - 13s 1s/step - loss: 20.1423 - accuracy: 0.9396 - f1_m: 0.9441 - precision_m: 0.9550 - recall_m: 0.9396 - val_loss: 24.0924 - val_accuracy: 0.4133 - val_f1_m: 0.0000e+00 - val_precision_m: 0.0000e+00 - val_recall_m: 0.0000e+00\n",
      "3/3 [==============================] - 1s 200ms/step - loss: 20.8066 - accuracy: 0.7733 - f1_m: 0.8355 - precision_m: 0.7191 - recall_m: 1.0000\n",
      "Epoch 1/50\n",
      "10/10 [==============================] - ETA: 0s - loss: 20.8735 - accuracy: 0.7926 - f1_m: 0.8172 - precision_m: 0.8187 - recall_m: 0.8436\n",
      "Epoch 00001: val_accuracy improved from -inf to 0.66216, saving model to saved_models/model_4.h5\n",
      "10/10 [==============================] - 24s 3s/step - loss: 20.8735 - accuracy: 0.7926 - f1_m: 0.8172 - precision_m: 0.8187 - recall_m: 0.8436 - val_loss: 21.1005 - val_accuracy: 0.6622 - val_f1_m: 0.8028 - val_precision_m: 0.6708 - val_recall_m: 1.0000\n",
      "Epoch 2/50\n",
      "10/10 [==============================] - ETA: 0s - loss: 20.5278 - accuracy: 0.9164 - f1_m: 0.9271 - precision_m: 0.9262 - recall_m: 0.9352\n",
      "Epoch 00002: val_accuracy did not improve from 0.66216\n",
      "10/10 [==============================] - 13s 1s/step - loss: 20.5278 - accuracy: 0.9164 - f1_m: 0.9271 - precision_m: 0.9262 - recall_m: 0.9352 - val_loss: 20.8552 - val_accuracy: 0.6622 - val_f1_m: 0.8151 - val_precision_m: 0.6937 - val_recall_m: 1.0000\n",
      "Epoch 3/50\n",
      "10/10 [==============================] - ETA: 0s - loss: 20.4767 - accuracy: 0.9365 - f1_m: 0.9486 - precision_m: 0.9646 - recall_m: 0.9373\n",
      "Epoch 00003: val_accuracy did not improve from 0.66216\n",
      "10/10 [==============================] - 13s 1s/step - loss: 20.4767 - accuracy: 0.9365 - f1_m: 0.9486 - precision_m: 0.9646 - recall_m: 0.9373 - val_loss: 20.9186 - val_accuracy: 0.3919 - val_f1_m: 0.1969 - val_precision_m: 1.0000 - val_recall_m: 0.1129\n",
      "Epoch 4/50\n",
      "10/10 [==============================] - ETA: 0s - loss: 20.3612 - accuracy: 0.9431 - f1_m: 0.9459 - precision_m: 0.9372 - recall_m: 0.9591\n",
      "Epoch 00004: val_accuracy did not improve from 0.66216\n",
      "10/10 [==============================] - 13s 1s/step - loss: 20.3612 - accuracy: 0.9431 - f1_m: 0.9459 - precision_m: 0.9372 - recall_m: 0.9591 - val_loss: 21.0684 - val_accuracy: 0.3378 - val_f1_m: 0.0000e+00 - val_precision_m: 0.0000e+00 - val_recall_m: 0.0000e+00\n",
      "Epoch 5/50\n",
      "10/10 [==============================] - ETA: 0s - loss: 20.4234 - accuracy: 0.9298 - f1_m: 0.9169 - precision_m: 0.8961 - recall_m: 0.9596\n",
      "Epoch 00005: val_accuracy did not improve from 0.66216\n",
      "10/10 [==============================] - 13s 1s/step - loss: 20.4234 - accuracy: 0.9298 - f1_m: 0.9169 - precision_m: 0.8961 - recall_m: 0.9596 - val_loss: 21.3190 - val_accuracy: 0.3378 - val_f1_m: 0.0000e+00 - val_precision_m: 0.0000e+00 - val_recall_m: 0.0000e+00\n",
      "Epoch 6/50\n",
      "10/10 [==============================] - ETA: 0s - loss: 20.3061 - accuracy: 0.9130 - f1_m: 0.9177 - precision_m: 0.9425 - recall_m: 0.9079\n",
      "Epoch 00006: val_accuracy did not improve from 0.66216\n",
      "10/10 [==============================] - 13s 1s/step - loss: 20.3061 - accuracy: 0.9130 - f1_m: 0.9177 - precision_m: 0.9425 - recall_m: 0.9079 - val_loss: 21.7945 - val_accuracy: 0.3378 - val_f1_m: 0.0000e+00 - val_precision_m: 0.0000e+00 - val_recall_m: 0.0000e+00\n",
      "Epoch 7/50\n",
      "10/10 [==============================] - ETA: 0s - loss: 20.1514 - accuracy: 0.9298 - f1_m: 0.9380 - precision_m: 0.9386 - recall_m: 0.9432\n",
      "Epoch 00007: val_accuracy did not improve from 0.66216\n",
      "10/10 [==============================] - 13s 1s/step - loss: 20.1514 - accuracy: 0.9298 - f1_m: 0.9380 - precision_m: 0.9386 - recall_m: 0.9432 - val_loss: 22.3909 - val_accuracy: 0.3378 - val_f1_m: 0.0000e+00 - val_precision_m: 0.0000e+00 - val_recall_m: 0.0000e+00\n",
      "3/3 [==============================] - 1s 209ms/step - loss: 21.1027 - accuracy: 0.6622 - f1_m: 0.8002 - precision_m: 0.6708 - recall_m: 1.0000\n",
      "Epoch 1/50\n",
      "10/10 [==============================] - ETA: 0s - loss: 21.6509 - accuracy: 0.6321 - f1_m: 0.6363 - precision_m: 0.7635 - recall_m: 0.6986\n",
      "Epoch 00001: val_accuracy improved from -inf to 0.60811, saving model to saved_models/model_5.h5\n",
      "10/10 [==============================] - 23s 2s/step - loss: 21.6509 - accuracy: 0.6321 - f1_m: 0.6363 - precision_m: 0.7635 - recall_m: 0.6986 - val_loss: 21.7438 - val_accuracy: 0.6081 - val_f1_m: 0.7414 - val_precision_m: 0.6062 - val_recall_m: 1.0000\n",
      "Epoch 2/50\n",
      "10/10 [==============================] - ETA: 0s - loss: 20.7663 - accuracy: 0.8227 - f1_m: 0.8502 - precision_m: 0.8755 - recall_m: 0.8533\n",
      "Epoch 00002: val_accuracy did not improve from 0.60811\n",
      "10/10 [==============================] - 13s 1s/step - loss: 20.7663 - accuracy: 0.8227 - f1_m: 0.8502 - precision_m: 0.8755 - recall_m: 0.8533 - val_loss: 21.3910 - val_accuracy: 0.6081 - val_f1_m: 0.7840 - val_precision_m: 0.6521 - val_recall_m: 1.0000\n",
      "Epoch 3/50\n",
      "10/10 [==============================] - ETA: 0s - loss: 20.6330 - accuracy: 0.8930 - f1_m: 0.9148 - precision_m: 0.8756 - recall_m: 0.9652\n",
      "Epoch 00003: val_accuracy did not improve from 0.60811\n",
      "10/10 [==============================] - 13s 1s/step - loss: 20.6330 - accuracy: 0.8930 - f1_m: 0.9148 - precision_m: 0.8756 - recall_m: 0.9652 - val_loss: 20.9748 - val_accuracy: 0.6081 - val_f1_m: 0.7631 - val_precision_m: 0.6292 - val_recall_m: 1.0000\n",
      "Epoch 4/50\n",
      "10/10 [==============================] - ETA: 0s - loss: 20.4737 - accuracy: 0.9231 - f1_m: 0.9316 - precision_m: 0.9555 - recall_m: 0.9159\n",
      "Epoch 00004: val_accuracy did not improve from 0.60811\n",
      "10/10 [==============================] - 13s 1s/step - loss: 20.4737 - accuracy: 0.9231 - f1_m: 0.9316 - precision_m: 0.9555 - recall_m: 0.9159 - val_loss: 20.9868 - val_accuracy: 0.2973 - val_f1_m: 0.0000e+00 - val_precision_m: 0.0000e+00 - val_recall_m: 0.0000e+00\n",
      "Epoch 5/50\n",
      "10/10 [==============================] - ETA: 0s - loss: 20.3665 - accuracy: 0.9431 - f1_m: 0.9504 - precision_m: 0.9427 - recall_m: 0.9616\n",
      "Epoch 00005: val_accuracy did not improve from 0.60811\n",
      "10/10 [==============================] - 13s 1s/step - loss: 20.3665 - accuracy: 0.9431 - f1_m: 0.9504 - precision_m: 0.9427 - recall_m: 0.9616 - val_loss: 21.6638 - val_accuracy: 0.3919 - val_f1_m: 0.0000e+00 - val_precision_m: 0.0000e+00 - val_recall_m: 0.0000e+00\n",
      "Epoch 6/50\n",
      "10/10 [==============================] - ETA: 0s - loss: 20.2738 - accuracy: 0.9465 - f1_m: 0.9588 - precision_m: 0.9656 - recall_m: 0.9561\n",
      "Epoch 00006: val_accuracy did not improve from 0.60811\n",
      "10/10 [==============================] - 13s 1s/step - loss: 20.2738 - accuracy: 0.9465 - f1_m: 0.9588 - precision_m: 0.9656 - recall_m: 0.9561 - val_loss: 22.5266 - val_accuracy: 0.3919 - val_f1_m: 0.0000e+00 - val_precision_m: 0.0000e+00 - val_recall_m: 0.0000e+00\n",
      "Epoch 7/50\n",
      "10/10 [==============================] - ETA: 0s - loss: 20.2294 - accuracy: 0.9465 - f1_m: 0.9536 - precision_m: 0.9390 - recall_m: 0.9724\n",
      "Epoch 00007: val_accuracy did not improve from 0.60811\n",
      "10/10 [==============================] - 13s 1s/step - loss: 20.2294 - accuracy: 0.9465 - f1_m: 0.9536 - precision_m: 0.9390 - recall_m: 0.9724 - val_loss: 23.4020 - val_accuracy: 0.3919 - val_f1_m: 0.0000e+00 - val_precision_m: 0.0000e+00 - val_recall_m: 0.0000e+00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/50\n",
      "10/10 [==============================] - ETA: 0s - loss: 20.1778 - accuracy: 0.9264 - f1_m: 0.9331 - precision_m: 0.9214 - recall_m: 0.9530\n",
      "Epoch 00008: val_accuracy did not improve from 0.60811\n",
      "10/10 [==============================] - 13s 1s/step - loss: 20.1778 - accuracy: 0.9264 - f1_m: 0.9331 - precision_m: 0.9214 - recall_m: 0.9530 - val_loss: 24.1135 - val_accuracy: 0.3919 - val_f1_m: 0.0000e+00 - val_precision_m: 0.0000e+00 - val_recall_m: 0.0000e+00\n",
      "3/3 [==============================] - 1s 206ms/step - loss: 21.7410 - accuracy: 0.6081 - f1_m: 0.7529 - precision_m: 0.6062 - recall_m: 1.0000\n"
     ]
    }
   ],
   "source": [
    "X =  np.concatenate([total_train_img,total_vali_img])\n",
    "y = np.concatenate([total_train_lbl,total_vali_lbl])\n",
    "\n",
    "VALIDATION_ACCURACY = []\n",
    "VALIDATION_LOSS = []\n",
    "\n",
    "save_dir = 'saved_models/'\n",
    "fold_var = 1\n",
    "\n",
    "kfold = KFold(n_splits=5, shuffle=True) \n",
    "# enumerate splits\n",
    "for train_index, valid_index in kfold.split(X, y):\n",
    "    #print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "    X_train, X_valid = X[train_index], X[valid_index]\n",
    "    y_train, y_valid = y[train_index], y[valid_index]\n",
    "\n",
    "    train_generator = train_datagen.flow(X_train, y_train, batch_size=32, shuffle=True)\n",
    "    valid_generator = train_datagen.flow(X_valid, y_valid, batch_size=32, shuffle=True)\n",
    "    model = build_model((img_height,img_width,3))\n",
    "    from keras import backend as K\n",
    "\n",
    "    def recall_m(y_true, y_pred):\n",
    "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "        recall = true_positives / (possible_positives + K.epsilon())\n",
    "        return recall\n",
    "\n",
    "    def precision_m(y_true, y_pred):\n",
    "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "        precision = true_positives / (predicted_positives + K.epsilon())\n",
    "        return precision\n",
    "\n",
    "    def f1_m(y_true, y_pred):\n",
    "        precision = precision_m(y_true, y_pred)\n",
    "        recall = recall_m(y_true, y_pred)\n",
    "        return 2*((precision*recall)/(precision+recall+K.epsilon()))\n",
    "\n",
    "    model.compile(loss='binary_crossentropy', \n",
    "           optimizer=tf.optimizers.Adam(learning_rate=0.00001),  \n",
    "           metrics=['accuracy',f1_m,precision_m, recall_m])\n",
    "    logdir = os.path.join(\"logs\", datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))  \n",
    "    my_callbacks = [\n",
    "    tf.keras.callbacks.EarlyStopping(patience=5),\n",
    "    tf.keras.callbacks.TensorBoard(logdir, histogram_freq=1),\n",
    "    tf.keras.callbacks.ModelCheckpoint(save_dir+get_model_name(fold_var), \n",
    "                                        monitor='val_accuracy', verbose=1, \n",
    "                                        save_best_only=True, mode='max')\n",
    "    ]\n",
    "\n",
    "    model.fit(train_generator, validation_data=valid_generator,\n",
    "                    callbacks=my_callbacks, epochs=50)\n",
    "    model.load_weights(\"saved_models/model_\"+str(fold_var)+\".h5\")\n",
    "\n",
    "    results = model.evaluate(valid_generator)\n",
    "    results = dict(zip(model.metrics_names,results))\n",
    "\n",
    "    VALIDATION_ACCURACY.append(results['accuracy'])\n",
    "    VALIDATION_LOSS.append(results['loss'])\n",
    "\n",
    "    tf.keras.backend.clear_session()\n",
    "\n",
    "    fold_var += 1\n",
    "   # loss, accuracy,Precision, Recall = model.evaluate(valid_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ba9d298d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/3 [==============================] - 1s 474ms/step - loss: 21.8281 - accuracy: 0.5914 - f1_m: 0.7435 - precision_m: 0.5934 - recall_m: 1.0000\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 4)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[0;32mIn [23]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m test_generator \u001b[38;5;241m=\u001b[39m test_datagen\u001b[38;5;241m.\u001b[39mflow(total_test_img,total_test_lbl)\n\u001b[0;32m----> 2\u001b[0m loss, accuracy,Precision, Recall \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mevaluate(test_generator)\n",
      "\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 4)"
     ]
    }
   ],
   "source": [
    "test_generator = test_datagen.flow(total_test_img,total_test_lbl)\n",
    "loss, accuracy,Precision, Recall = model.evaluate(test_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aba7337",
   "metadata": {},
   "outputs": [],
   "source": [
    "VALIDATION_ACCURACY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4cdb3e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(VALIDATION_ACCURACY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "961f0ad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# summarize feature map shapes\n",
    "for i in range(len(model.layers)):\n",
    "    layer = model.layers[i]\n",
    "    # check for convolutional layer\n",
    "    if 'conv' not in layer.name:\n",
    "        continue\n",
    "    # summarize output shape\n",
    "    print(i, layer.name, layer.output.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80f9a58b",
   "metadata": {},
   "outputs": [],
   "source": [
    "ixs = [3, 7, 9, 13, 15]\n",
    "outputs = [model.layers[i+1].output for i in ixs]\n",
    "model1 = tf.keras.Model(inputs=model.inputs, outputs=outputs)\n",
    "model1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0b46444",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dir = pathlib.Path('/home/jaliyah/data/single_cell//')\n",
    "img_path = os.path.join(test_dir, 'slide eleven (464).jpg')\n",
    "\n",
    "test_img = keras.preprocessing.image.load_img(img_path, target_size=(img_height, img_width))\n",
    "img_array = keras.preprocessing.image.img_to_array(test_img)\n",
    "img_array = tf.expand_dims(img_array, 0)  # Create batch axis\n",
    "print(img_array.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36e92007",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8415f66f",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(feature_maps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "841a7bb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as pyplot\n",
    "square = 8\n",
    "\n",
    "f = plt.figure(figsize=(15,15))\n",
    "ix = 1\n",
    "for _ in range(square):\n",
    "    for _ in range(square):\n",
    "        # specify subplot and turn of axis\n",
    "        ax = pyplot.subplot(square, square, ix)\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "        # plot filter channel in grayscale\n",
    "        pyplot.imshow(np.squeeze(feature_maps[1])[:, :, ix-1])\n",
    "        ix += 1\n",
    "# show the figure\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ac160b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "filters, biases = model.layers[9].get_weights()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60eec825",
   "metadata": {},
   "outputs": [],
   "source": [
    "f_min, f_max = filters.min(), filters.max()\n",
    "filters = (filters - f_min) / (f_max - f_min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccbfe6a5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "n_filters, ix = 6, 1\n",
    "for i in range(n_filters):\n",
    "    # get the filter\n",
    "    f = filters[:, :, :, i]\n",
    "    # plot each channel separately\n",
    "    for j in range(3):\n",
    "        # specify subplot and turn of axis\n",
    "        ax = pyplot.subplot(n_filters, 3, ix)\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "        # plot filter channel in grayscale\n",
    "        pyplot.imshow(f[:, :, j], cmap='gray')\n",
    "        ix += 1\n",
    "# show the figure\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1db77592",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dir = pathlib.Path('/home/jaliyah/data/single_cell//')\n",
    "img_path = os.path.join(test_dir, 'slide eleven (313).jpg')\n",
    "\n",
    "test_img = keras.preprocessing.image.load_img(img_path, target_size=(img_height, img_width))\n",
    "img_array = keras.preprocessing.image.img_to_array(test_img)\n",
    "img_array = tf.expand_dims(img_array, 0)  # Create batch axis\n",
    "print(img_array.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "043d5e38",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_img_array(img_path, size):\n",
    "    # `img` is a PIL image of size 299x299\n",
    "    img = keras.preprocessing.image.load_img(img_path, target_size=size)\n",
    "    # `array` is a float32 Numpy array of shape (299, 299, 3)\n",
    "    array = keras.preprocessing.image.img_to_array(img)\n",
    "    # We add a dimension to transform our array into a \"batch\"\n",
    "    # of size (1, 299, 299, 3)\n",
    "    array = np.expand_dims(array, axis=0)\n",
    "    return array\n",
    "\n",
    "\n",
    "def make_gradcam_heatmap(img_array, model, last_conv_layer_name, pred_index=None):\n",
    "    # First, we create a model that maps the input image to the activations\n",
    "    # of the last conv layer as well as the output predictions\n",
    "    grad_model = tf.keras.models.Model(\n",
    "        [model.inputs], [model.get_layer(last_conv_layer_name).output, model.output]\n",
    "    )\n",
    "\n",
    "    # Then, we compute the gradient of the top predicted class for our input image\n",
    "    # with respect to the activations of the last conv layer\n",
    "    with tf.GradientTape() as tape:\n",
    "        last_conv_layer_output, preds = grad_model(img_array)\n",
    "        if pred_index is None:\n",
    "            pred_index = tf.argmax(preds[0])\n",
    "        class_channel = preds[:, pred_index]\n",
    "\n",
    "    # This is the gradient of the output neuron (top predicted or chosen)\n",
    "    # with regard to the output feature map of the last conv layer\n",
    "    grads = tape.gradient(class_channel, last_conv_layer_output)\n",
    "\n",
    "    # This is a vector where each entry is the mean intensity of the gradient\n",
    "    # over a specific feature map channel\n",
    "    pooled_grads = tf.reduce_mean(grads, axis=(0, 1, 2))\n",
    "\n",
    "    # We multiply each channel in the feature map array\n",
    "    # by \"how important this channel is\" with regard to the top predicted class\n",
    "    # then sum all the channels to obtain the heatmap class activation\n",
    "    last_conv_layer_output = last_conv_layer_output[0]\n",
    "    heatmap = last_conv_layer_output @ pooled_grads[..., tf.newaxis]\n",
    "    heatmap = tf.squeeze(heatmap)\n",
    "\n",
    "    # For visualization purpose, we will also normalize the heatmap between 0 & 1\n",
    "    heatmap = tf.maximum(heatmap, 0) / tf.math.reduce_max(heatmap)\n",
    "    return heatmap.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9098e8c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_array.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29ca2641",
   "metadata": {},
   "outputs": [],
   "source": [
    "last_conv_layer_name = 'conv2d_4'\n",
    "heatmap = make_gradcam_heatmap(img_array, model, last_conv_layer_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8de9397",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.matshow(heatmap)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a7de8ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.predict(img_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49457724",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_and_display_gradcam(img_path, heatmap, cam_path=\"cam.jpg\", alpha=0.4):\n",
    "    # Load the original image\n",
    "    img = keras.preprocessing.image.load_img(img_path)\n",
    "    img = keras.preprocessing.image.img_to_array(img)\n",
    "\n",
    "    # Rescale heatmap to a range 0-255\n",
    "    heatmap = np.uint8(255 * heatmap)\n",
    "\n",
    "    # Use jet colormap to colorize heatmap\n",
    "    jet = cm.get_cmap(\"jet\")\n",
    "\n",
    "    # Use RGB values of the colormap\n",
    "    jet_colors = jet(np.arange(256))[:, :3]\n",
    "    jet_heatmap = jet_colors[heatmap]\n",
    "\n",
    "    # Create an image with RGB colorized heatmap\n",
    "    jet_heatmap = keras.preprocessing.image.array_to_img(jet_heatmap)\n",
    "    jet_heatmap = jet_heatmap.resize((img.shape[1], img.shape[0]))\n",
    "    jet_heatmap = keras.preprocessing.image.img_to_array(jet_heatmap)\n",
    "\n",
    "    # Superimpose the heatmap on original image\n",
    "    superimposed_img = jet_heatmap * alpha + img\n",
    "    superimposed_img = keras.preprocessing.image.array_to_img(superimposed_img)\n",
    "\n",
    "    # Save the superimposed image\n",
    "    superimposed_img.save(cam_path)\n",
    "\n",
    "    # Display Grad CAM1\n",
    "    display(Image(cam_path))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61d7cef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af33d56d",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_and_display_gradcam(img_path, heatmap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21e128fd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
